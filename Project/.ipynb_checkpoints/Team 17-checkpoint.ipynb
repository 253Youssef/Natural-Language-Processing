{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools as itertools\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split,validation_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemWord(sen):\n",
    "    tokens = word_tokenize(sen)\n",
    "    result = ''\n",
    "    ps = PorterStemmer()\n",
    "    for wordToken in tokens:\n",
    "        result = result + ' ' + ps.stem(wordToken)\n",
    "    return result\n",
    "\n",
    "def checkEnglish(sen, english_vocab):\n",
    "    totalCount = len(sen.split())\n",
    "    counter = 0\n",
    "    for word in sen:\n",
    "        if word in english_vocab:\n",
    "            counter = counter + 1\n",
    "    \n",
    "    return((counter/totalCount) >= 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Implementation: No Filtrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Bayes Before Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.99      0.80      1826\n",
      "     neutral       0.73      0.14      0.23       611\n",
      "    positive       0.88      0.17      0.28       491\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2928\n",
      "   macro avg       0.76      0.43      0.44      2928\n",
      "weighted avg       0.72      0.68      0.59      2928\n",
      "\n",
      "K-Neighbours Before Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.85      0.81      1826\n",
      "     neutral       0.48      0.42      0.45       611\n",
      "    positive       0.67      0.56      0.61       491\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2928\n",
      "   macro avg       0.64      0.61      0.62      2928\n",
      "weighted avg       0.70      0.71      0.70      2928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Before Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.92      0.84      1826\n",
      "     neutral       0.56      0.37      0.45       611\n",
      "    positive       0.73      0.50      0.59       491\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      2928\n",
      "   macro avg       0.69      0.60      0.63      2928\n",
      "weighted avg       0.72      0.74      0.72      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Remove URLs in tweet texts\n",
    "df['text'] = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), axis=1)\n",
    "\n",
    "# Stem the words\n",
    "df['text'] = df.apply(lambda row: stemWord(row.text), axis=1)\n",
    "\n",
    "# Store the Target variable\n",
    "Y = df['airline_sentiment']\n",
    "\n",
    "# Filter the reviewText column in the dataset\n",
    "X = df['text']\n",
    "\n",
    "# Split the dataset into training and testing sets with a ration of 0.8:0.2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Define tfidf vectorizer with maximum of 40000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clfM1 = MultinomialNB()\n",
    "\n",
    "clfM1.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfM1.predict(X_test_transformed)\n",
    "print('Multinomial Bayes Before Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfK1 = KNeighborsClassifier()\n",
    "\n",
    "clfK1.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfK1.predict(X_test_transformed)\n",
    "print('K-Neighbours Before Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfR1 = RandomForestClassifier()\n",
    "\n",
    "clfR1.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfR1.predict(X_test_transformed)\n",
    "print('Random Forest Before Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Number 4: Filtrations on the Airline Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counts for each rating:\n",
      " negative    8810\n",
      "neutral     2603\n",
      "positive    1998\n",
      "Name: airline_sentiment, dtype: int64\n",
      "\n",
      "Shape after equal distribution:  (5994, 5)\n",
      "Multinomial Bayes After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.81       385\n",
      "     neutral       0.75      0.58      0.66       395\n",
      "    positive       0.77      0.80      0.79       419\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1199\n",
      "   macro avg       0.76      0.76      0.75      1199\n",
      "weighted avg       0.76      0.76      0.75      1199\n",
      "\n",
      "K-Neighbours After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.71      0.69       385\n",
      "     neutral       0.60      0.59      0.60       395\n",
      "    positive       0.72      0.69      0.71       419\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      1199\n",
      "   macro avg       0.66      0.66      0.66      1199\n",
      "weighted avg       0.66      0.66      0.66      1199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.88      0.78       385\n",
      "     neutral       0.64      0.58      0.61       395\n",
      "    positive       0.75      0.65      0.70       419\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1199\n",
      "   macro avg       0.70      0.71      0.70      1199\n",
      "weighted avg       0.70      0.70      0.70      1199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Remove URLs in tweet texts\n",
    "df['text'] = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), axis=1)\n",
    "\n",
    "# Choose tweets with length less greater than or equal to 20 characters\n",
    "df['length'] = df.apply(lambda row: len(row.text), axis=1)\n",
    "df = df[df.length>20]\n",
    "\n",
    "# Check english words\n",
    "english_vocab = [w.lower() for w in nltk.corpus.words.words()]\n",
    "english_freq = FreqDist(english_vocab)\n",
    "english_freq = english_freq.most_common(2000)\n",
    "commonWords, _ = zip(*english_freq)\n",
    "commonWords = commonWords[:2000]\n",
    "df['english'] = df.apply(lambda row: checkEnglish(row.text, commonWords), axis=1)\n",
    "df = df[df.english == True]\n",
    "\n",
    "# Remove retweets\n",
    "df = df[~df.text.str.contains('RT')]\n",
    "\n",
    "# Choose selected columns \n",
    "selectedColummns = ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'text']\n",
    "df = df[selectedColummns]\n",
    "\n",
    "# Fill NAs with given values\n",
    "values = {'negativereason': '', 'negativereason_confidence': 0}\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "# Choose tweets with confidence more than 50%\n",
    "df = df[ ((df['airline_sentiment_confidence'] >= 0.5) | (df['negativereason_confidence'] >= 0.5)) ]\n",
    "\n",
    "# Stem the words\n",
    "df['text'] = df.apply(lambda row: stemWord(row.text), axis=1)\n",
    "# Form tfidf vecctorizer from the tweet texts\n",
    "text = df['text']\n",
    "fullTfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "Xtransformed = fullTfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "# Create dataframe from tfidf vectorizer\n",
    "tfidf = pd.DataFrame(Xtransformed.toarray(), columns=fullTfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Compute cosine similarity for the tweets\n",
    "cosSimilarity = (cosine_similarity(tfidf)).tolist()\n",
    "\n",
    "# Set empty set and indices used for looping over all tweets\n",
    "indices = set()\n",
    "rowNumber = 0\n",
    "itemNumber = 0\n",
    "\n",
    "# Loop over all tweets and their cosine similarity with other tweets\n",
    "for row in cosSimilarity:\n",
    "    itemNumber = 0\n",
    "    # Loop over all cosine similarities of the other tweets with the tweet being inspected in the outer loop\n",
    "    for other in row:\n",
    "        # Check that we are not comparing the tweet's cosine similarity with itself\n",
    "        if (rowNumber != itemNumber):\n",
    "            if (other >= 0.9):\n",
    "                # Add the index of the other tweets if the similarity exceeds 90%\n",
    "                indices.add(itemNumber)\n",
    "        itemNumber = itemNumber + 1\n",
    "    rowNumber = rowNumber + 1\n",
    "\n",
    "# Remove the tweets with the indices present in the indices set\n",
    "df = df.drop(df.index[list(indices)])\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Unique counts for each rating:\\n',df.airline_sentiment.value_counts())\n",
    "# Set the number of rows to select from each sentiment class the minimum number of tweets present in a sentiment class\n",
    "numberOfRows = (df.airline_sentiment.value_counts()).min()\n",
    "# Select a number of rows from each sentiment class\n",
    "df = pd.concat([(df.loc[df['airline_sentiment'] == 'neutral']).head(numberOfRows), \n",
    "                (df.loc[df['airline_sentiment'] == 'negative']).head(numberOfRows),\n",
    "                (df.loc[df['airline_sentiment'] == 'positive']).head(numberOfRows)\n",
    "                ])\n",
    "print('\\nShape after equal distribution: ',df.shape)\n",
    "\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the Target variable\n",
    "Y = df['airline_sentiment']\n",
    "\n",
    "# Filter the reviewText column in the dataset\n",
    "X = df['text']\n",
    "\n",
    "# Split the dataset into training and testing sets with a ration of 0.8:0.2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Define tfidf vectorizer with maximum of 40000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clfM2 = MultinomialNB()\n",
    "\n",
    "clfM2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfM2.predict(X_test_transformed)\n",
    "print('Multinomial Bayes After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfK2 = KNeighborsClassifier()\n",
    "\n",
    "clfK2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfK2.predict(X_test_transformed)\n",
    "print('K-Neighbours After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfR2 = RandomForestClassifier()\n",
    "\n",
    "clfR2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfR2.predict(X_test_transformed)\n",
    "print('Random Forest After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Number 3: Sentiment140 Dataset + Filtrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counts for each rating:\n",
      " 4    9519\n",
      "0    9493\n",
      "Name: polarity, dtype: int64\n",
      "\n",
      "Shape after equal distribution:  (18986, 8)\n",
      "Multinomial Bayes After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      1896\n",
      "           4       0.76      0.67      0.71      1902\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      3798\n",
      "   macro avg       0.73      0.73      0.73      3798\n",
      "weighted avg       0.73      0.73      0.73      3798\n",
      "\n",
      "K-Neighbours After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.69      0.65      1896\n",
      "           4       0.65      0.59      0.62      1902\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      3798\n",
      "   macro avg       0.64      0.64      0.64      3798\n",
      "weighted avg       0.64      0.64      0.64      3798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      1896\n",
      "           4       0.73      0.61      0.66      1902\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      3798\n",
      "   macro avg       0.70      0.69      0.69      3798\n",
      "weighted avg       0.70      0.69      0.69      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', engine = 'python', \n",
    "                 names=['polarity','id','date','query','user','text'])\n",
    "\n",
    "# Convert polarity feature to string rather than int\n",
    "df['polarity'] = df['polarity'].astype('str')\n",
    "\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Choose 1st 20000 tweets to prevent memory errors\n",
    "df = df.head(20000)\n",
    "\n",
    "# Remove URLs in tweet texts\n",
    "df['text'] = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), axis=1)\n",
    "\n",
    "# Choose tweets with length less greater than or equal to 20 characters\n",
    "df['length'] = df.apply(lambda row: len(row.text), axis=1)\n",
    "df = df[df.length>20]\n",
    "\n",
    "# Check english words\n",
    "english_vocab = [w.lower() for w in nltk.corpus.words.words()]\n",
    "english_freq = FreqDist(english_vocab)\n",
    "english_freq = english_freq.most_common(2000)\n",
    "commonWords, _ = zip(*english_freq)\n",
    "commonWords = commonWords[:2000]\n",
    "df['english'] = df.apply(lambda row: checkEnglish(row.text, commonWords), axis=1)\n",
    "df = df[df.english == True]\n",
    "\n",
    "# Remove retweets\n",
    "df = df[~df.text.str.contains('RT')]\n",
    "\n",
    "# Stem the words\n",
    "df['text'] = df.apply(lambda row: stemWord(row.text), axis=1)\n",
    "# Form tfidf vecctorizer from the tweet texts\n",
    "text = df['text']\n",
    "fullTfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "Xtransformed = fullTfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "# Create dataframe from tfidf vectorizer\n",
    "tfidf = pd.DataFrame(Xtransformed.toarray(), columns=fullTfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Compute cosine similarity for the tweets\n",
    "cosSimilarity = (cosine_similarity(tfidf)).tolist()\n",
    "\n",
    "# Set empty set and indices used for looping over all tweets\n",
    "indices = set()\n",
    "rowNumber = 0\n",
    "itemNumber = 0\n",
    "\n",
    "# Loop over all tweets and their cosine similarity with other tweets\n",
    "for row in cosSimilarity:\n",
    "    itemNumber = 0\n",
    "    # Loop over all cosine similarities of the other tweets with the tweet being inspected in the outer loop\n",
    "    for other in row:\n",
    "        # Check that we are not comparing the tweet's cosine similarity with itself\n",
    "        if (rowNumber != itemNumber):\n",
    "            if (other >= 0.9):\n",
    "                # Add the index of the other tweets if the similarity exceeds 90%\n",
    "                indices.add(itemNumber)\n",
    "        itemNumber = itemNumber + 1\n",
    "    rowNumber = rowNumber + 1\n",
    "\n",
    "# Remove the tweets with the indices present in the indices set\n",
    "df = df.drop(df.index[list(indices)])\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Unique counts for each rating:\\n',df.polarity.value_counts())\n",
    "# Set the number of rows to select from each sentiment class the minimum number of tweets present in a sentiment class\n",
    "numberOfRows = (df.polarity.value_counts()).min()\n",
    "# Select a number of rows from each sentiment class\n",
    "df = pd.concat([(df.loc[df['polarity'] == '2']).head(numberOfRows), \n",
    "                (df.loc[df['polarity'] == '0']).head(numberOfRows),\n",
    "                (df.loc[df['polarity'] == '4']).head(numberOfRows)\n",
    "                ])\n",
    "print('\\nShape after equal distribution: ',df.shape)\n",
    "\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the Target variable\n",
    "Y = df['polarity']\n",
    "\n",
    "# Filter the reviewText column in the dataset\n",
    "X = df['text']\n",
    "\n",
    "# Split the dataset into training and testing sets with a ration of 0.8:0.2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Define tfidf vectorizer with maximum of 40000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clfM2 = MultinomialNB()\n",
    "\n",
    "clfM2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfM2.predict(X_test_transformed)\n",
    "print('Multinomial Bayes After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfK2 = KNeighborsClassifier()\n",
    "\n",
    "clfK2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfK2.predict(X_test_transformed)\n",
    "print('K-Neighbours After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfR2 = RandomForestClassifier()\n",
    "\n",
    "clfR2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfR2.predict(X_test_transformed)\n",
    "print('Random Forest After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
