{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools as itertools\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split,validation_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemWord(sen):\n",
    "    tokens = word_tokenize(sen)\n",
    "    result = ''\n",
    "    ps = PorterStemmer()\n",
    "    for wordToken in tokens:\n",
    "        result = result + ' ' + ps.stem(wordToken)\n",
    "    return result\n",
    "\n",
    "def checkEnglish(sen, english_vocab):\n",
    "    totalCount = len(sen.split())\n",
    "    counter = 0\n",
    "    for word in sen:\n",
    "        if word in english_vocab:\n",
    "            counter = counter + 1\n",
    "    \n",
    "    return((counter/totalCount) >= 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Implementation: No Filtrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Bayes Before Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.99      0.80      1826\n",
      "     neutral       0.73      0.14      0.23       611\n",
      "    positive       0.88      0.17      0.28       491\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2928\n",
      "   macro avg       0.76      0.43      0.44      2928\n",
      "weighted avg       0.72      0.68      0.59      2928\n",
      "\n",
      "K-Neighbours Before Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.85      0.81      1826\n",
      "     neutral       0.48      0.42      0.45       611\n",
      "    positive       0.67      0.56      0.61       491\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2928\n",
      "   macro avg       0.64      0.61      0.62      2928\n",
      "weighted avg       0.70      0.71      0.70      2928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Before Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.92      0.84      1826\n",
      "     neutral       0.56      0.37      0.45       611\n",
      "    positive       0.73      0.50      0.59       491\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      2928\n",
      "   macro avg       0.69      0.60      0.63      2928\n",
      "weighted avg       0.72      0.74      0.72      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Remove URLs in tweet texts\n",
    "df['text'] = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), axis=1)\n",
    "\n",
    "# Stem the words\n",
    "df['text'] = df.apply(lambda row: stemWord(row.text), axis=1)\n",
    "\n",
    "# Store the Target variable\n",
    "Y = df['airline_sentiment']\n",
    "\n",
    "# Filter the reviewText column in the dataset\n",
    "X = df['text']\n",
    "\n",
    "# Split the dataset into training and testing sets with a ration of 0.8:0.2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Define tfidf vectorizer with maximum of 40000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clfM1 = MultinomialNB()\n",
    "\n",
    "clfM1.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfM1.predict(X_test_transformed)\n",
    "print('Multinomial Bayes Before Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfK1 = KNeighborsClassifier()\n",
    "\n",
    "clfK1.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfK1.predict(X_test_transformed)\n",
    "print('K-Neighbours Before Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfR1 = RandomForestClassifier()\n",
    "\n",
    "clfR1.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfR1.predict(X_test_transformed)\n",
    "print('Random Forest Before Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Number 4: Filtrations on the Airline Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counts for each rating:\n",
      " negative    8810\n",
      "neutral     2603\n",
      "positive    1998\n",
      "Name: airline_sentiment, dtype: int64\n",
      "\n",
      "Shape after equal distribution:  (5994, 5)\n",
      "Multinomial Bayes After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.81       385\n",
      "     neutral       0.75      0.58      0.66       395\n",
      "    positive       0.77      0.80      0.79       419\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1199\n",
      "   macro avg       0.76      0.76      0.75      1199\n",
      "weighted avg       0.76      0.76      0.75      1199\n",
      "\n",
      "K-Neighbours After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.71      0.69       385\n",
      "     neutral       0.60      0.59      0.60       395\n",
      "    positive       0.72      0.69      0.71       419\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      1199\n",
      "   macro avg       0.66      0.66      0.66      1199\n",
      "weighted avg       0.66      0.66      0.66      1199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.88      0.78       385\n",
      "     neutral       0.64      0.58      0.61       395\n",
      "    positive       0.75      0.65      0.70       419\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1199\n",
      "   macro avg       0.70      0.71      0.70      1199\n",
      "weighted avg       0.70      0.70      0.70      1199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Remove URLs in tweet texts\n",
    "df['text'] = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), axis=1)\n",
    "\n",
    "# Choose tweets with length less greater than or equal to 20 characters\n",
    "df['length'] = df.apply(lambda row: len(row.text), axis=1)\n",
    "df = df[df.length>20]\n",
    "\n",
    "# Check english words\n",
    "english_vocab = [w.lower() for w in nltk.corpus.words.words()]\n",
    "english_freq = FreqDist(english_vocab)\n",
    "english_freq = english_freq.most_common(2000)\n",
    "commonWords, _ = zip(*english_freq)\n",
    "commonWords = commonWords[:2000]\n",
    "df['english'] = df.apply(lambda row: checkEnglish(row.text, commonWords), axis=1)\n",
    "df = df[df.english == True]\n",
    "\n",
    "# Remove retweets\n",
    "df = df[~df.text.str.contains('RT')]\n",
    "\n",
    "# Choose selected columns \n",
    "selectedColummns = ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'text']\n",
    "df = df[selectedColummns]\n",
    "\n",
    "# Fill NAs with given values\n",
    "values = {'negativereason': '', 'negativereason_confidence': 0}\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "# Choose tweets with confidence more than 50%\n",
    "df = df[ ((df['airline_sentiment_confidence'] >= 0.5) | (df['negativereason_confidence'] >= 0.5)) ]\n",
    "\n",
    "# Stem the words\n",
    "df['text'] = df.apply(lambda row: stemWord(row.text), axis=1)\n",
    "# Form tfidf vecctorizer from the tweet texts\n",
    "text = df['text']\n",
    "fullTfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "Xtransformed = fullTfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "# Create dataframe from tfidf vectorizer\n",
    "tfidf = pd.DataFrame(Xtransformed.toarray(), columns=fullTfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Compute cosine similarity for the tweets\n",
    "cosSimilarity = (cosine_similarity(tfidf)).tolist()\n",
    "\n",
    "# Set empty set and indices used for looping over all tweets\n",
    "indices = set()\n",
    "rowNumber = 0\n",
    "itemNumber = 0\n",
    "\n",
    "# Loop over all tweets and their cosine similarity with other tweets\n",
    "for row in cosSimilarity:\n",
    "    itemNumber = 0\n",
    "    # Loop over all cosine similarities of the other tweets with the tweet being inspected in the outer loop\n",
    "    for other in row:\n",
    "        # Check that we are not comparing the tweet's cosine similarity with itself\n",
    "        if (rowNumber != itemNumber):\n",
    "            if (other >= 0.9):\n",
    "                # Add the index of the other tweets if the similarity exceeds 90%\n",
    "                indices.add(itemNumber)\n",
    "        itemNumber = itemNumber + 1\n",
    "    rowNumber = rowNumber + 1\n",
    "\n",
    "# Remove the tweets with the indices present in the indices set\n",
    "df = df.drop(df.index[list(indices)])\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Unique counts for each rating:\\n',df.airline_sentiment.value_counts())\n",
    "# Set the number of rows to select from each sentiment class the minimum number of tweets present in a sentiment class\n",
    "numberOfRows = (df.airline_sentiment.value_counts()).min()\n",
    "# Select a number of rows from each sentiment class\n",
    "df = pd.concat([(df.loc[df['airline_sentiment'] == 'neutral']).head(numberOfRows), \n",
    "                (df.loc[df['airline_sentiment'] == 'negative']).head(numberOfRows),\n",
    "                (df.loc[df['airline_sentiment'] == 'positive']).head(numberOfRows)\n",
    "                ])\n",
    "print('\\nShape after equal distribution: ',df.shape)\n",
    "\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the Target variable\n",
    "Y = df['airline_sentiment']\n",
    "\n",
    "# Filter the reviewText column in the dataset\n",
    "X = df['text']\n",
    "\n",
    "# Split the dataset into training and testing sets with a ration of 0.8:0.2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Define tfidf vectorizer with maximum of 40000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clfM2 = MultinomialNB()\n",
    "\n",
    "clfM2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfM2.predict(X_test_transformed)\n",
    "print('Multinomial Bayes After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfK2 = KNeighborsClassifier()\n",
    "\n",
    "clfK2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfK2.predict(X_test_transformed)\n",
    "print('K-Neighbours After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfR2 = RandomForestClassifier()\n",
    "\n",
    "clfR2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfR2.predict(X_test_transformed)\n",
    "print('Random Forest After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Number 3: Sentiment140 Dataset + Filtrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1581466"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', engine = 'python', \n",
    "                 names=['polarity','id','date','query','user','text'])\n",
    "\n",
    "# Convert polarity feature to string rather than int\n",
    "df['polarity'] = df['polarity'].astype('str')\n",
    "\n",
    "len(df.text.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I need a hug '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[54].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>1467821455</td>\n",
       "      <td>Mon Apr 06 22:22:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CiaraRenee</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>1467841832</td>\n",
       "      <td>Mon Apr 06 22:27:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bgoers</td>\n",
       "      <td>I'm so cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>1467863684</td>\n",
       "      <td>Mon Apr 06 22:33:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DjGundam</td>\n",
       "      <td>Awwh babs... you look so sad underneith that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "      <td>1467872175</td>\n",
       "      <td>Mon Apr 06 22:35:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>edsed</td>\n",
       "      <td>I still can't find my keys.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>1467872759</td>\n",
       "      <td>Mon Apr 06 22:35:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Augustina22CA</td>\n",
       "      <td>im lonely  keep me company! 22 female, california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0</td>\n",
       "      <td>1467880442</td>\n",
       "      <td>Mon Apr 06 22:38:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>iCalvin</td>\n",
       "      <td>Haven't tweeted nearly all day  Posted my webs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0</td>\n",
       "      <td>1467900545</td>\n",
       "      <td>Mon Apr 06 22:43:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>brookes4402</td>\n",
       "      <td>homework....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0</td>\n",
       "      <td>1467901500</td>\n",
       "      <td>Mon Apr 06 22:43:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thegeach</td>\n",
       "      <td>feeling down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>1467912842</td>\n",
       "      <td>Mon Apr 06 22:46:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KimberlyKane</td>\n",
       "      <td>@danadearmond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0</td>\n",
       "      <td>1467930017</td>\n",
       "      <td>Mon Apr 06 22:51:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Glycel</td>\n",
       "      <td>stuck at home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0</td>\n",
       "      <td>1467949746</td>\n",
       "      <td>Mon Apr 06 22:57:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xShyGirlx</td>\n",
       "      <td>At work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>1467963477</td>\n",
       "      <td>Mon Apr 06 23:01:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Augustina22</td>\n",
       "      <td>im lonely  keep me company! 22 female, new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0</td>\n",
       "      <td>1467969001</td>\n",
       "      <td>Mon Apr 06 23:02:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Trissies</td>\n",
       "      <td>wants to cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0</td>\n",
       "      <td>1467993608</td>\n",
       "      <td>Mon Apr 06 23:09:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mscproductions</td>\n",
       "      <td>Can't sleep again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0</td>\n",
       "      <td>1467995217</td>\n",
       "      <td>Mon Apr 06 23:10:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChrisVanPatten</td>\n",
       "      <td>A little sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0</td>\n",
       "      <td>1468013308</td>\n",
       "      <td>Mon Apr 06 23:15:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sineadfitz</td>\n",
       "      <td>It's only tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>1468031172</td>\n",
       "      <td>Mon Apr 06 23:21:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>flashbrother</td>\n",
       "      <td>work again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0</td>\n",
       "      <td>1468053611</td>\n",
       "      <td>Mon Apr 06 23:28:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mariejamora</td>\n",
       "      <td>@hellobebe I also send some updates in plurk b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>0</td>\n",
       "      <td>1468062309</td>\n",
       "      <td>Mon Apr 06 23:30:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mizzoutigress09</td>\n",
       "      <td>Can't fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>0</td>\n",
       "      <td>1468100580</td>\n",
       "      <td>Mon Apr 06 23:42:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cristygarza</td>\n",
       "      <td>good night swetdreamss to everyonee   and jare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>0</td>\n",
       "      <td>1468101990</td>\n",
       "      <td>Mon Apr 06 23:43:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MellyFed</td>\n",
       "      <td>headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>0</td>\n",
       "      <td>1468107426</td>\n",
       "      <td>Mon Apr 06 23:45:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>iain20</td>\n",
       "      <td>Up early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>0</td>\n",
       "      <td>1468115720</td>\n",
       "      <td>Mon Apr 06 23:48:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>WarholGirl</td>\n",
       "      <td>@ientje89 aw i'm fine too thanks! yeah i miss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>0</td>\n",
       "      <td>1468131748</td>\n",
       "      <td>Mon Apr 06 23:53:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>steveslee</td>\n",
       "      <td>@gordonchiu You're one letter alway!    Korean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>0</td>\n",
       "      <td>1468133697</td>\n",
       "      <td>Mon Apr 06 23:54:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HeartStarDot</td>\n",
       "      <td>I'm in pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>0</td>\n",
       "      <td>1468161883</td>\n",
       "      <td>Tue Apr 07 00:03:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>warrenaissance</td>\n",
       "      <td>I found my MADDEN '08!  in '09  ...oh well, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>0</td>\n",
       "      <td>1468185939</td>\n",
       "      <td>Tue Apr 07 00:10:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xtinasteimel23</td>\n",
       "      <td>i cant sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>0</td>\n",
       "      <td>1468224250</td>\n",
       "      <td>Tue Apr 07 00:23:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lindseykanno</td>\n",
       "      <td>@friendlypharm  too bad it's true, for the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>0</td>\n",
       "      <td>1468238111</td>\n",
       "      <td>Tue Apr 07 00:28:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>andy2boyz</td>\n",
       "      <td>woke up too early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>0</td>\n",
       "      <td>1468244282</td>\n",
       "      <td>Tue Apr 07 00:30:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SamBusbySleeps</td>\n",
       "      <td>Work today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597753</th>\n",
       "      <td>4</td>\n",
       "      <td>2193007684</td>\n",
       "      <td>Tue Jun 16 07:52:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bamboobangaa</td>\n",
       "      <td>Good morning world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597792</th>\n",
       "      <td>4</td>\n",
       "      <td>2193009230</td>\n",
       "      <td>Tue Jun 16 07:52:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xxwillowundead</td>\n",
       "      <td>eating apples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597820</th>\n",
       "      <td>4</td>\n",
       "      <td>2193030138</td>\n",
       "      <td>Tue Jun 16 07:54:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Shortylisious</td>\n",
       "      <td>lunch time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598219</th>\n",
       "      <td>4</td>\n",
       "      <td>2193122486</td>\n",
       "      <td>Tue Jun 16 08:01:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>s2siil</td>\n",
       "      <td>Going home.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598287</th>\n",
       "      <td>4</td>\n",
       "      <td>2193152928</td>\n",
       "      <td>Tue Jun 16 08:04:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SuPeR_mEgAn</td>\n",
       "      <td>@mikeyway http://twitpic.com/7j4ra - where can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598289</th>\n",
       "      <td>4</td>\n",
       "      <td>2193152976</td>\n",
       "      <td>Tue Jun 16 08:04:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SuPeR_mEgAn</td>\n",
       "      <td>@mikeyway http://twitpic.com/7j4ra - where can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598444</th>\n",
       "      <td>4</td>\n",
       "      <td>2193181413</td>\n",
       "      <td>Tue Jun 16 08:06:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>faith_phoenix</td>\n",
       "      <td>@RealBillBailey gutted i couldnt get tickets f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598546</th>\n",
       "      <td>4</td>\n",
       "      <td>2193190384</td>\n",
       "      <td>Tue Jun 16 08:07:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>youscareme</td>\n",
       "      <td>@buckhollywood omg! im such a loser! how did i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598625</th>\n",
       "      <td>4</td>\n",
       "      <td>2193223303</td>\n",
       "      <td>Tue Jun 16 08:09:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hvhuynh</td>\n",
       "      <td>goodmornin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598780</th>\n",
       "      <td>4</td>\n",
       "      <td>2193278017</td>\n",
       "      <td>Tue Jun 16 08:14:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>janiecwales</td>\n",
       "      <td>oh dear HH is back   please twitter do somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598957</th>\n",
       "      <td>4</td>\n",
       "      <td>2193318305</td>\n",
       "      <td>Tue Jun 16 08:17:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>majaricious</td>\n",
       "      <td>@KhloeKardashian Definitely my Mom. And Angeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598959</th>\n",
       "      <td>4</td>\n",
       "      <td>2193318357</td>\n",
       "      <td>Tue Jun 16 08:17:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ummsistweets</td>\n",
       "      <td>cedar point with my biffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599004</th>\n",
       "      <td>4</td>\n",
       "      <td>2193320445</td>\n",
       "      <td>Tue Jun 16 08:17:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>anbanan313</td>\n",
       "      <td>Good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599129</th>\n",
       "      <td>4</td>\n",
       "      <td>2193346564</td>\n",
       "      <td>Tue Jun 16 08:20:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>analiliajonas</td>\n",
       "      <td>so happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599234</th>\n",
       "      <td>4</td>\n",
       "      <td>2193400835</td>\n",
       "      <td>Tue Jun 16 08:24:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Dannymassacur</td>\n",
       "      <td>Good morning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599279</th>\n",
       "      <td>4</td>\n",
       "      <td>2193402891</td>\n",
       "      <td>Tue Jun 16 08:24:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>regilek</td>\n",
       "      <td>Good morning sunshine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599298</th>\n",
       "      <td>4</td>\n",
       "      <td>2193403830</td>\n",
       "      <td>Tue Jun 16 08:24:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lauren__x33</td>\n",
       "      <td>english exam went okay        revising for fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599306</th>\n",
       "      <td>4</td>\n",
       "      <td>2193404064</td>\n",
       "      <td>Tue Jun 16 08:24:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xoxcaitlynxox</td>\n",
       "      <td>Watching Hannah Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599326</th>\n",
       "      <td>4</td>\n",
       "      <td>2193404782</td>\n",
       "      <td>Tue Jun 16 08:24:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Xplizzit</td>\n",
       "      <td>GOOD MORNING!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599353</th>\n",
       "      <td>4</td>\n",
       "      <td>2193427113</td>\n",
       "      <td>Tue Jun 16 08:26:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>parawhore182</td>\n",
       "      <td>Im busy. Drooling. Over this video. ;)  but ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599354</th>\n",
       "      <td>4</td>\n",
       "      <td>2193427134</td>\n",
       "      <td>Tue Jun 16 08:26:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>parawhore182</td>\n",
       "      <td>Im busy. Drooling. Over this video. ;)  but ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599377</th>\n",
       "      <td>4</td>\n",
       "      <td>2193427910</td>\n",
       "      <td>Tue Jun 16 08:26:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ummsistweets</td>\n",
       "      <td>cedar point with my biffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599385</th>\n",
       "      <td>4</td>\n",
       "      <td>2193428118</td>\n",
       "      <td>Tue Jun 16 08:26:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bubbles00</td>\n",
       "      <td>finally finished typing!!!! Woohoooo  , still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599440</th>\n",
       "      <td>4</td>\n",
       "      <td>2193451289</td>\n",
       "      <td>Tue Jun 16 08:28:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>faiznurdavid</td>\n",
       "      <td>@fanafatin see, @misschimichanga tweet u to jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599450</th>\n",
       "      <td>4</td>\n",
       "      <td>2193451876</td>\n",
       "      <td>Tue Jun 16 08:28:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>asiatikah</td>\n",
       "      <td>Good morning!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599501</th>\n",
       "      <td>4</td>\n",
       "      <td>2193453784</td>\n",
       "      <td>Tue Jun 16 08:28:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_cammi_</td>\n",
       "      <td>getting used to twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599531</th>\n",
       "      <td>4</td>\n",
       "      <td>2193474515</td>\n",
       "      <td>Tue Jun 16 08:30:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>majaricious</td>\n",
       "      <td>@KhloeKardashian Definitely my Mom. And Angeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599678</th>\n",
       "      <td>4</td>\n",
       "      <td>2193503347</td>\n",
       "      <td>Tue Jun 16 08:32:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ShaneLemmon</td>\n",
       "      <td>goodmorning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599768</th>\n",
       "      <td>4</td>\n",
       "      <td>2193528075</td>\n",
       "      <td>Tue Jun 16 08:34:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rach3lizabeth</td>\n",
       "      <td>Good morning everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599931</th>\n",
       "      <td>4</td>\n",
       "      <td>2193576442</td>\n",
       "      <td>Tue Jun 16 08:38:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>emma_b_xox</td>\n",
       "      <td>Had an injection today. Not fun  the rrst of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26968 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity          id                          date     query  \\\n",
       "54             0  1467821455  Mon Apr 06 22:22:32 PDT 2009  NO_QUERY   \n",
       "128            0  1467841832  Mon Apr 06 22:27:55 PDT 2009  NO_QUERY   \n",
       "213            0  1467863684  Mon Apr 06 22:33:35 PDT 2009  NO_QUERY   \n",
       "230            0  1467872175  Mon Apr 06 22:35:50 PDT 2009  NO_QUERY   \n",
       "238            0  1467872759  Mon Apr 06 22:35:59 PDT 2009  NO_QUERY   \n",
       "275            0  1467880442  Mon Apr 06 22:38:04 PDT 2009  NO_QUERY   \n",
       "357            0  1467900545  Mon Apr 06 22:43:31 PDT 2009  NO_QUERY   \n",
       "364            0  1467901500  Mon Apr 06 22:43:49 PDT 2009  NO_QUERY   \n",
       "398            0  1467912842  Mon Apr 06 22:46:53 PDT 2009  NO_QUERY   \n",
       "465            0  1467930017  Mon Apr 06 22:51:48 PDT 2009  NO_QUERY   \n",
       "545            0  1467949746  Mon Apr 06 22:57:28 PDT 2009  NO_QUERY   \n",
       "599            0  1467963477  Mon Apr 06 23:01:15 PDT 2009  NO_QUERY   \n",
       "626            0  1467969001  Mon Apr 06 23:02:45 PDT 2009  NO_QUERY   \n",
       "729            0  1467993608  Mon Apr 06 23:09:51 PDT 2009  NO_QUERY   \n",
       "737            0  1467995217  Mon Apr 06 23:10:17 PDT 2009  NO_QUERY   \n",
       "819            0  1468013308  Mon Apr 06 23:15:35 PDT 2009  NO_QUERY   \n",
       "870            0  1468031172  Mon Apr 06 23:21:02 PDT 2009  NO_QUERY   \n",
       "989            0  1468053611  Mon Apr 06 23:28:09 PDT 2009  NO_QUERY   \n",
       "1015           0  1468062309  Mon Apr 06 23:30:44 PDT 2009  NO_QUERY   \n",
       "1177           0  1468100580  Mon Apr 06 23:42:57 PDT 2009  NO_QUERY   \n",
       "1189           0  1468101990  Mon Apr 06 23:43:26 PDT 2009  NO_QUERY   \n",
       "1205           0  1468107426  Mon Apr 06 23:45:15 PDT 2009  NO_QUERY   \n",
       "1254           0  1468115720  Mon Apr 06 23:48:00 PDT 2009  NO_QUERY   \n",
       "1333           0  1468131748  Mon Apr 06 23:53:22 PDT 2009  NO_QUERY   \n",
       "1350           0  1468133697  Mon Apr 06 23:54:03 PDT 2009  NO_QUERY   \n",
       "1477           0  1468161883  Tue Apr 07 00:03:10 PDT 2009  NO_QUERY   \n",
       "1561           0  1468185939  Tue Apr 07 00:10:52 PDT 2009  NO_QUERY   \n",
       "1752           0  1468224250  Tue Apr 07 00:23:49 PDT 2009  NO_QUERY   \n",
       "1807           0  1468238111  Tue Apr 07 00:28:35 PDT 2009  NO_QUERY   \n",
       "1830           0  1468244282  Tue Apr 07 00:30:40 PDT 2009  NO_QUERY   \n",
       "...          ...         ...                           ...       ...   \n",
       "1597753        4  2193007684  Tue Jun 16 07:52:04 PDT 2009  NO_QUERY   \n",
       "1597792        4  2193009230  Tue Jun 16 07:52:11 PDT 2009  NO_QUERY   \n",
       "1597820        4  2193030138  Tue Jun 16 07:54:00 PDT 2009  NO_QUERY   \n",
       "1598219        4  2193122486  Tue Jun 16 08:01:40 PDT 2009  NO_QUERY   \n",
       "1598287        4  2193152928  Tue Jun 16 08:04:06 PDT 2009  NO_QUERY   \n",
       "1598289        4  2193152976  Tue Jun 16 08:04:07 PDT 2009  NO_QUERY   \n",
       "1598444        4  2193181413  Tue Jun 16 08:06:24 PDT 2009  NO_QUERY   \n",
       "1598546        4  2193190384  Tue Jun 16 08:07:09 PDT 2009  NO_QUERY   \n",
       "1598625        4  2193223303  Tue Jun 16 08:09:52 PDT 2009  NO_QUERY   \n",
       "1598780        4  2193278017  Tue Jun 16 08:14:22 PDT 2009  NO_QUERY   \n",
       "1598957        4  2193318305  Tue Jun 16 08:17:40 PDT 2009  NO_QUERY   \n",
       "1598959        4  2193318357  Tue Jun 16 08:17:40 PDT 2009  NO_QUERY   \n",
       "1599004        4  2193320445  Tue Jun 16 08:17:50 PDT 2009  NO_QUERY   \n",
       "1599129        4  2193346564  Tue Jun 16 08:20:01 PDT 2009  NO_QUERY   \n",
       "1599234        4  2193400835  Tue Jun 16 08:24:29 PDT 2009  NO_QUERY   \n",
       "1599279        4  2193402891  Tue Jun 16 08:24:39 PDT 2009  NO_QUERY   \n",
       "1599298        4  2193403830  Tue Jun 16 08:24:44 PDT 2009  NO_QUERY   \n",
       "1599306        4  2193404064  Tue Jun 16 08:24:45 PDT 2009  NO_QUERY   \n",
       "1599326        4  2193404782  Tue Jun 16 08:24:49 PDT 2009  NO_QUERY   \n",
       "1599353        4  2193427113  Tue Jun 16 08:26:38 PDT 2009  NO_QUERY   \n",
       "1599354        4  2193427134  Tue Jun 16 08:26:38 PDT 2009  NO_QUERY   \n",
       "1599377        4  2193427910  Tue Jun 16 08:26:43 PDT 2009  NO_QUERY   \n",
       "1599385        4  2193428118  Tue Jun 16 08:26:44 PDT 2009  NO_QUERY   \n",
       "1599440        4  2193451289  Tue Jun 16 08:28:37 PDT 2009  NO_QUERY   \n",
       "1599450        4  2193451876  Tue Jun 16 08:28:39 PDT 2009  NO_QUERY   \n",
       "1599501        4  2193453784  Tue Jun 16 08:28:49 PDT 2009  NO_QUERY   \n",
       "1599531        4  2193474515  Tue Jun 16 08:30:28 PDT 2009  NO_QUERY   \n",
       "1599678        4  2193503347  Tue Jun 16 08:32:47 PDT 2009  NO_QUERY   \n",
       "1599768        4  2193528075  Tue Jun 16 08:34:48 PDT 2009  NO_QUERY   \n",
       "1599931        4  2193576442  Tue Jun 16 08:38:45 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "54            CiaraRenee                                      I need a hug   \n",
       "128               bgoers                                       I'm so cold   \n",
       "213             DjGundam  Awwh babs... you look so sad underneith that s...  \n",
       "230                edsed                       I still can't find my keys.   \n",
       "238        Augustina22CA  im lonely  keep me company! 22 female, california  \n",
       "275              iCalvin  Haven't tweeted nearly all day  Posted my webs...  \n",
       "357          brookes4402                                      homework....   \n",
       "364             thegeach                                      feeling down   \n",
       "398         KimberlyKane                                     @danadearmond   \n",
       "465               Glycel                                     stuck at home   \n",
       "545            xShyGirlx                                           At work   \n",
       "599          Augustina22    im lonely  keep me company! 22 female, new york  \n",
       "626             Trissies                                      wants to cry   \n",
       "729       mscproductions                                 Can't sleep again   \n",
       "737       ChrisVanPatten                                      A little sad   \n",
       "819           sineadfitz                                 It's only tuesday   \n",
       "870         flashbrother                                        work again   \n",
       "989          mariejamora  @hellobebe I also send some updates in plurk b...  \n",
       "1015     mizzoutigress09                                 Can't fall asleep   \n",
       "1177         cristygarza  good night swetdreamss to everyonee   and jare...  \n",
       "1189            MellyFed                                          headache   \n",
       "1205              iain20                                          Up early   \n",
       "1254          WarholGirl  @ientje89 aw i'm fine too thanks! yeah i miss ...  \n",
       "1333           steveslee  @gordonchiu You're one letter alway!    Korean...  \n",
       "1350        HeartStarDot                                       I'm in pain   \n",
       "1477      warrenaissance  I found my MADDEN '08!  in '09  ...oh well, I ...  \n",
       "1561      xtinasteimel23                                      i cant sleep   \n",
       "1752        lindseykanno  @friendlypharm  too bad it's true, for the mos...  \n",
       "1807           andy2boyz                                 woke up too early   \n",
       "1830      SamBusbySleeps                                        Work today   \n",
       "...                  ...                                                ...  \n",
       "1597753     bamboobangaa                                Good morning world   \n",
       "1597792   xxwillowundead                                     eating apples   \n",
       "1597820    Shortylisious                                        lunch time   \n",
       "1598219           s2siil                                       Going home.   \n",
       "1598287      SuPeR_mEgAn  @mikeyway http://twitpic.com/7j4ra - where can...  \n",
       "1598289      SuPeR_mEgAn  @mikeyway http://twitpic.com/7j4ra - where can...  \n",
       "1598444    faith_phoenix  @RealBillBailey gutted i couldnt get tickets f...  \n",
       "1598546       youscareme  @buckhollywood omg! im such a loser! how did i...  \n",
       "1598625          hvhuynh                                        goodmornin   \n",
       "1598780      janiecwales  oh dear HH is back   please twitter do somethi...  \n",
       "1598957      majaricious  @KhloeKardashian Definitely my Mom. And Angeli...  \n",
       "1598959     ummsistweets                         cedar point with my biffs   \n",
       "1599004       anbanan313                                      Good morning   \n",
       "1599129    analiliajonas                                          so happy   \n",
       "1599234    Dannymassacur                                     Good morning.   \n",
       "1599279          regilek                             Good morning sunshine   \n",
       "1599298      lauren__x33  english exam went okay        revising for fre...  \n",
       "1599306    xoxcaitlynxox                           Watching Hannah Montana   \n",
       "1599326         Xplizzit                                    GOOD MORNING!!   \n",
       "1599353     parawhore182  Im busy. Drooling. Over this video. ;)  but ye...  \n",
       "1599354     parawhore182  Im busy. Drooling. Over this video. ;)  but ye...  \n",
       "1599377     ummsistweets                         cedar point with my biffs   \n",
       "1599385        bubbles00  finally finished typing!!!! Woohoooo  , still ...  \n",
       "1599440     faiznurdavid  @fanafatin see, @misschimichanga tweet u to jo...  \n",
       "1599450        asiatikah                                     Good morning!   \n",
       "1599501          _cammi_                           getting used to twitter   \n",
       "1599531      majaricious  @KhloeKardashian Definitely my Mom. And Angeli...  \n",
       "1599678      ShaneLemmon                                       goodmorning   \n",
       "1599768    rach3lizabeth                            Good morning everyone.   \n",
       "1599931       emma_b_xox  Had an injection today. Not fun  the rrst of t...  \n",
       "\n",
       "[26968 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>1467821455</td>\n",
       "      <td>Mon Apr 06 22:22:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CiaraRenee</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35285</th>\n",
       "      <td>0</td>\n",
       "      <td>1565156502</td>\n",
       "      <td>Mon Apr 20 06:03:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>1Song</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84168</th>\n",
       "      <td>0</td>\n",
       "      <td>1753529442</td>\n",
       "      <td>Sun May 10 02:05:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>liedra</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92500</th>\n",
       "      <td>0</td>\n",
       "      <td>1760069887</td>\n",
       "      <td>Sun May 10 20:35:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DarianFroseth</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103005</th>\n",
       "      <td>0</td>\n",
       "      <td>1795358529</td>\n",
       "      <td>Thu May 14 07:37:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AlexandraTheSpy</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109872</th>\n",
       "      <td>0</td>\n",
       "      <td>1824686179</td>\n",
       "      <td>Sun May 17 02:28:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>krystalcyo</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141634</th>\n",
       "      <td>0</td>\n",
       "      <td>1881307284</td>\n",
       "      <td>Fri May 22 04:16:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Suff0cat</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230755</th>\n",
       "      <td>0</td>\n",
       "      <td>1978810915</td>\n",
       "      <td>Sun May 31 00:32:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>melombardo</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242916</th>\n",
       "      <td>0</td>\n",
       "      <td>1981423369</td>\n",
       "      <td>Sun May 31 09:07:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xshmodie</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279499</th>\n",
       "      <td>0</td>\n",
       "      <td>1991805662</td>\n",
       "      <td>Mon Jun 01 07:41:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Jaydaboo18</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326559</th>\n",
       "      <td>0</td>\n",
       "      <td>2008259075</td>\n",
       "      <td>Tue Jun 02 13:51:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CookieGotGame</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490513</th>\n",
       "      <td>0</td>\n",
       "      <td>2183428022</td>\n",
       "      <td>Mon Jun 15 14:29:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Livestarbucks</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529893</th>\n",
       "      <td>0</td>\n",
       "      <td>2195679618</td>\n",
       "      <td>Tue Jun 16 11:27:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gfromtherock</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541009</th>\n",
       "      <td>0</td>\n",
       "      <td>2199942649</td>\n",
       "      <td>Tue Jun 16 18:13:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>charmlala</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718253</th>\n",
       "      <td>0</td>\n",
       "      <td>2260242023</td>\n",
       "      <td>Sat Jun 20 18:54:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>nanananancy</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748181</th>\n",
       "      <td>0</td>\n",
       "      <td>2284149996</td>\n",
       "      <td>Mon Jun 22 13:31:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BriarRose83</td>\n",
       "      <td>I need a hug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       polarity          id                          date     query  \\\n",
       "54            0  1467821455  Mon Apr 06 22:22:32 PDT 2009  NO_QUERY   \n",
       "35285         0  1565156502  Mon Apr 20 06:03:00 PDT 2009  NO_QUERY   \n",
       "84168         0  1753529442  Sun May 10 02:05:57 PDT 2009  NO_QUERY   \n",
       "92500         0  1760069887  Sun May 10 20:35:44 PDT 2009  NO_QUERY   \n",
       "103005        0  1795358529  Thu May 14 07:37:55 PDT 2009  NO_QUERY   \n",
       "109872        0  1824686179  Sun May 17 02:28:47 PDT 2009  NO_QUERY   \n",
       "141634        0  1881307284  Fri May 22 04:16:17 PDT 2009  NO_QUERY   \n",
       "230755        0  1978810915  Sun May 31 00:32:57 PDT 2009  NO_QUERY   \n",
       "242916        0  1981423369  Sun May 31 09:07:20 PDT 2009  NO_QUERY   \n",
       "279499        0  1991805662  Mon Jun 01 07:41:45 PDT 2009  NO_QUERY   \n",
       "326559        0  2008259075  Tue Jun 02 13:51:40 PDT 2009  NO_QUERY   \n",
       "490513        0  2183428022  Mon Jun 15 14:29:33 PDT 2009  NO_QUERY   \n",
       "529893        0  2195679618  Tue Jun 16 11:27:29 PDT 2009  NO_QUERY   \n",
       "541009        0  2199942649  Tue Jun 16 18:13:11 PDT 2009  NO_QUERY   \n",
       "718253        0  2260242023  Sat Jun 20 18:54:13 PDT 2009  NO_QUERY   \n",
       "748181        0  2284149996  Mon Jun 22 13:31:34 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user           text  \n",
       "54           CiaraRenee  I need a hug   \n",
       "35285             1Song  I need a hug   \n",
       "84168            liedra  I need a hug   \n",
       "92500     DarianFroseth  I need a hug   \n",
       "103005  AlexandraTheSpy  I need a hug   \n",
       "109872       krystalcyo  I need a hug   \n",
       "141634         Suff0cat  I need a hug   \n",
       "230755       melombardo  I need a hug   \n",
       "242916         xshmodie  I need a hug   \n",
       "279499       Jaydaboo18  I need a hug   \n",
       "326559    CookieGotGame  I need a hug   \n",
       "490513    Livestarbucks  I need a hug   \n",
       "529893     gfromtherock  I need a hug   \n",
       "541009        charmlala  I need a hug   \n",
       "718253      nanananancy  I need a hug   \n",
       "748181      BriarRose83  I need a hug   "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df[\"text\"]\n",
    "display(df[text.isin(text[text.duplicated()])])\n",
    "df.loc[df.text == (df.iloc[54].text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [polarity, id, date, query, user, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counts for each rating:\n",
      " 4    9622\n",
      "0    9417\n",
      "Name: polarity, dtype: int64\n",
      "\n",
      "Shape after equal distribution:  (18834, 8)\n",
      "Multinomial Bayes After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.74      1871\n",
      "           4       0.75      0.66      0.70      1896\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      3767\n",
      "   macro avg       0.72      0.72      0.72      3767\n",
      "weighted avg       0.72      0.72      0.72      3767\n",
      "\n",
      "K-Neighbours After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68      1871\n",
      "           4       0.70      0.42      0.53      1896\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      3767\n",
      "   macro avg       0.64      0.62      0.61      3767\n",
      "weighted avg       0.64      0.62      0.60      3767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest After Filtration Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70      1871\n",
      "           4       0.71      0.65      0.68      1896\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      3767\n",
      "   macro avg       0.69      0.69      0.69      3767\n",
      "weighted avg       0.69      0.69      0.69      3767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the tweets\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', engine = 'python', \n",
    "                 names=['polarity','id','date','query','user','text'])\n",
    "\n",
    "# Convert polarity feature to string rather than int\n",
    "df['polarity'] = df['polarity'].astype('str')\n",
    "\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Choose 1st 20000 tweets to prevent memory errors\n",
    "df = df.head(20000)\n",
    "\n",
    "# Remove URLs in tweet texts\n",
    "df['text'] = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), axis=1)\n",
    "\n",
    "# Choose tweets with length less greater than or equal to 20 characters\n",
    "df['length'] = df.apply(lambda row: len(row.text), axis=1)\n",
    "df = df[df.length>20]\n",
    "\n",
    "# Check english words\n",
    "english_vocab = [w.lower() for w in nltk.corpus.words.words()]\n",
    "english_freq = FreqDist(english_vocab)\n",
    "english_freq = english_freq.most_common(2000)\n",
    "commonWords, _ = zip(*english_freq)\n",
    "commonWords = commonWords[:2000]\n",
    "df['english'] = df.apply(lambda row: checkEnglish(row.text, commonWords), axis=1)\n",
    "df = df[df.english == True]\n",
    "\n",
    "# Remove retweets\n",
    "df = df[~df.text.str.contains('RT')]\n",
    "\n",
    "# Stem the words\n",
    "df['text'] = df.apply(lambda row: stemWord(row.text), axis=1)\n",
    "# Form tfidf vecctorizer from the tweet texts\n",
    "text = df['text']\n",
    "fullTfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "Xtransformed = fullTfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "# Create dataframe from tfidf vectorizer\n",
    "tfidf = pd.DataFrame(Xtransformed.toarray(), columns=fullTfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Compute cosine similarity for the tweets\n",
    "cosSimilarity = (cosine_similarity(tfidf)).tolist()\n",
    "\n",
    "# Set empty set and indices used for looping over all tweets\n",
    "indices = set()\n",
    "rowNumber = 0\n",
    "itemNumber = 0\n",
    "\n",
    "# Loop over all tweets and their cosine similarity with other tweets\n",
    "for row in cosSimilarity:\n",
    "    itemNumber = 0\n",
    "    # Loop over all cosine similarities of the other tweets with the tweet being inspected in the outer loop\n",
    "    for other in row:\n",
    "        # Check that we are not comparing the tweet's cosine similarity with itself\n",
    "        if (rowNumber != itemNumber):\n",
    "            if (other >= 0.9):\n",
    "                # Add the index of the other tweets if the similarity exceeds 90%\n",
    "                indices.add(itemNumber)\n",
    "        itemNumber = itemNumber + 1\n",
    "    rowNumber = rowNumber + 1\n",
    "\n",
    "# Remove the tweets with the indices present in the indices set\n",
    "df = df.drop(df.index[list(indices)])\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Unique counts for each rating:\\n',df.polarity.value_counts())\n",
    "# Set the number of rows to select from each sentiment class the minimum number of tweets present in a sentiment class\n",
    "numberOfRows = (df.polarity.value_counts()).min()\n",
    "# Select a number of rows from each sentiment class\n",
    "df = pd.concat([(df.loc[df['polarity'] == '2']).head(numberOfRows), \n",
    "                (df.loc[df['polarity'] == '0']).head(numberOfRows),\n",
    "                (df.loc[df['polarity'] == '4']).head(numberOfRows)\n",
    "                ])\n",
    "print('\\nShape after equal distribution: ',df.shape)\n",
    "\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the Target variable\n",
    "Y = df['polarity']\n",
    "\n",
    "# Filter the reviewText column in the dataset\n",
    "X = df['text']\n",
    "\n",
    "# Split the dataset into training and testing sets with a ration of 0.8:0.2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Define tfidf vectorizer with maximum of 40000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 40000,sublinear_tf=False, analyzer='word', stop_words='english',strip_accents='ascii')\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clfM2 = MultinomialNB()\n",
    "\n",
    "clfM2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfM2.predict(X_test_transformed)\n",
    "print('Multinomial Bayes After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfK2 = KNeighborsClassifier()\n",
    "\n",
    "clfK2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfK2.predict(X_test_transformed)\n",
    "print('K-Neighbours After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))\n",
    "\n",
    "clfR2 = RandomForestClassifier()\n",
    "\n",
    "clfR2.fit(X_train_transformed, Y_train)\n",
    "Y_pred = clfR2.predict(X_test_transformed)\n",
    "print('Random Forest After Filtration Report:\\n', metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
